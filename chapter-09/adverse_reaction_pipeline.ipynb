{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "98dd10a0"
   },
   "source": [
    "# Adverse event clustering pipeline\n",
    "\n",
    "####This exercise is part of *Chapter 9* in the book *Applied Machine Learning for Healthcare and Lifesciences on AWS*. Make sure you have completed the steps as outlined in the prerequisites section of *Chapter 9* to successfully complete this exercise.\n",
    "\n",
    "In this notebook we will will create a SageMaker pipeline to preprocess the data, train a model, and register the model in SageMaker model registry. Here are the details of each step in the pipeline:\n",
    "\n",
    "1. The preprocessing step is carried out in a custom container. During preprocessing, we download raw data, sample 100 rows from it, extract the top five medical conditions from them and vectorize those conditions. \n",
    "2. In the training step, we use a SageMaker `scikitlearn` container to train a clustering model in SageMaker.\n",
    "3. In the final step, we register the model in SageMaker model registry.\n",
    "\n",
    "\n",
    "Let's start by making sure we have the correct version of SageMaker installed. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "83d7a0d7",
    "outputId": "19233ca5-147f-4847-cd2b-08f8c24834de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker>=2.99.0 in /opt/conda/lib/python3.7/site-packages (2.107.0)\n",
      "Requirement already satisfied: attrs<22,>=20.3.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.99.0) (21.4.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.99.0) (1.21.6)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.99.0) (0.2.9)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.99.0) (1.0.1)\n",
      "Requirement already satisfied: boto3<2.0,>=1.20.21 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.99.0) (1.24.62)\n",
      "Requirement already satisfied: protobuf3-to-dict<1.0,>=0.1.5 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.99.0) (0.1.5)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.99.0) (0.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.99.0) (20.1)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.99.0) (1.3.5)\n",
      "Requirement already satisfied: protobuf<4.0,>=3.1 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.99.0) (3.20.1)\n",
      "Requirement already satisfied: importlib-metadata<5.0,>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.99.0) (4.12.0)\n",
      "Requirement already satisfied: botocore<1.28.0,>=1.27.62 in /opt/conda/lib/python3.7/site-packages (from boto3<2.0,>=1.20.21->sagemaker>=2.99.0) (1.27.62)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from boto3<2.0,>=1.20.21->sagemaker>=2.99.0) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from boto3<2.0,>=1.20.21->sagemaker>=2.99.0) (0.6.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker>=2.99.0) (3.8.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker>=2.99.0) (4.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->sagemaker>=2.99.0) (2.4.6)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->sagemaker>=2.99.0) (1.14.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->sagemaker>=2.99.0) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->sagemaker>=2.99.0) (2.8.1)\n",
      "Requirement already satisfied: ppft>=1.7.6.5 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker>=2.99.0) (1.7.6.5)\n",
      "Requirement already satisfied: multiprocess>=0.70.13 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker>=2.99.0) (0.70.13)\n",
      "Requirement already satisfied: pox>=0.3.1 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker>=2.99.0) (0.3.1)\n",
      "Requirement already satisfied: dill>=0.3.5.1 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker>=2.99.0) (0.3.5.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.7/site-packages (from botocore<1.28.0,>=1.27.62->boto3<2.0,>=1.20.21->sagemaker>=2.99.0) (1.26.12)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install \"sagemaker>=2.99.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8f321a75"
   },
   "source": [
    "Next, we import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "7f801df8"
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.session import Session\n",
    "import boto3\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8523449d"
   },
   "source": [
    "We also set some variables that we will use later in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "b8ba6595",
    "outputId": "50348436-759a-4833-8032-428d7cbb69d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training input/output will be stored in sagemaker-us-east-1-485822383573/chapter9/data\n",
      "\n",
      "IAM Role: arn:aws:iam::485822383573:role/AmazonSageMaker-ExecutionRole-biobert\n",
      "\n",
      "IAM Role Name: AmazonSageMaker-ExecutionRole-biobert\n"
     ]
    }
   ],
   "source": [
    "sagemaker_session=sagemaker.Session()\n",
    "pipeline_session = PipelineSession()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "role = get_execution_role()\n",
    "role_name = role.split(\"/\")[len(role.split(\"/\"))-1]\n",
    "prefix = 'chapter9/data'\n",
    "\n",
    "print('Training input/output will be stored in {}/{}'.format(bucket, prefix))\n",
    "print('\\nIAM Role: {}'.format(role))\n",
    "print('\\nIAM Role Name: {}'.format(role_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now attach the comprehend medical policy to our role so that we can call the comprehend medical APIs during the preprocessing step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attaching policies...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'f2419faa-253e-4440-a5e6-b56e575c3cb3',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'f2419faa-253e-4440-a5e6-b56e575c3cb3',\n",
       "   'content-type': 'text/xml',\n",
       "   'content-length': '212',\n",
       "   'date': 'Fri, 04 Nov 2022 23:07:27 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Attaching policies...\")\n",
    "iam = boto3.client(\"iam\")\n",
    "iam.attach_role_policy(\n",
    "    RoleName = role_name,\n",
    "    PolicyArn = \"arn:aws:iam::aws:policy/ComprehendMedicalFullAccess\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bbb7121e"
   },
   "source": [
    "Let's start by examining our preprocessing script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c1a6664f",
    "outputId": "aa3b0e66-e42a-43cb-82f0-cd300f95f412"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcsv\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mwget\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mzipfile\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mboto3\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtime\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mpreprocessing\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Normalizer\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mfeature_extraction\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mtext\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m TfidfVectorizer\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n",
      "\n",
      "parser = argparse.ArgumentParser()\n",
      "parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--bucket\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m)\n",
      "parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--region\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m)\n",
      "args = parser.parse_args()\n",
      "\n",
      "bucket=args.bucket\n",
      "cm = boto3.client(\u001b[33m'\u001b[39;49;00m\u001b[33mcomprehendmedical\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,region_name=args.region)\n",
      "s3_client = boto3.client(\u001b[33m'\u001b[39;49;00m\u001b[33ms3\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,region_name=args.region)\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m os.path.exists(\u001b[33m'\u001b[39;49;00m\u001b[33mdata\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)==\u001b[34mFalse\u001b[39;49;00m:\n",
      "    os.mkdir(\u001b[33m'\u001b[39;49;00m\u001b[33mdata\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "file_url = \u001b[33m'\u001b[39;49;00m\u001b[33mhttps://archive.ics.uci.edu/ml/machine-learning-databases/00462/drugsCom_raw.zip\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "dest_file = \u001b[33m'\u001b[39;49;00m\u001b[33mdata/drugsCom_raw.zip\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "\n",
      "\u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mDownloading source files...\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "wget.download(file_url, dest_file)\n",
      "\n",
      "\u001b[34mwith\u001b[39;49;00m zipfile.ZipFile(\u001b[33m'\u001b[39;49;00m\u001b[33mdata/drugsCom_raw.zip\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mr\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m zip_ref:\n",
      "    zip_ref.extractall(\u001b[33m'\u001b[39;49;00m\u001b[33mdata\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "os.remove(\u001b[33m'\u001b[39;49;00m\u001b[33mdata/drugsCom_raw.zip\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "orig_list = \u001b[36mlist\u001b[39;49;00m()\n",
      "\u001b[34mfor\u001b[39;49;00m filename \u001b[35min\u001b[39;49;00m os.listdir(\u001b[33m'\u001b[39;49;00m\u001b[33mdata\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m):\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mdata/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m+filename) \u001b[34mas\u001b[39;49;00m csvfile:\n",
      "        myreader = csv.reader(csvfile, delimiter=\u001b[33m'\u001b[39;49;00m\u001b[33m\\t\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        \u001b[34mfor\u001b[39;49;00m row \u001b[35min\u001b[39;49;00m myreader:\n",
      "            \u001b[34mif\u001b[39;49;00m row[\u001b[34m0\u001b[39;49;00m] == \u001b[33m'\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n",
      "                \u001b[34mcontinue\u001b[39;49;00m\n",
      "            \u001b[34melse\u001b[39;49;00m:\n",
      "                orig_list.append({\n",
      "                    \u001b[33m'\u001b[39;49;00m\u001b[33mid\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: row[\u001b[34m0\u001b[39;49;00m],\n",
      "                    \u001b[33m'\u001b[39;49;00m\u001b[33mdrugName\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: row[\u001b[34m1\u001b[39;49;00m],\n",
      "                    \u001b[33m'\u001b[39;49;00m\u001b[33mcondition\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: row[\u001b[34m2\u001b[39;49;00m],\n",
      "                    \u001b[33m'\u001b[39;49;00m\u001b[33mreview\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: row[\u001b[34m3\u001b[39;49;00m]\n",
      "                })\n",
      "\n",
      "    \n",
      "\u001b[34mif\u001b[39;49;00m os.path.exists(\u001b[33m'\u001b[39;49;00m\u001b[33mprocessed_data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)==\u001b[34mFalse\u001b[39;49;00m:\n",
      "    os.mkdir(\u001b[33m'\u001b[39;49;00m\u001b[33mprocessed_data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \n",
      "raw_df=pd.DataFrame.from_records(orig_list)\n",
      "raw_df.to_csv(\u001b[33m'\u001b[39;49;00m\u001b[33mprocessed_data/raw_df.csv\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, index=\u001b[34mFalse\u001b[39;49;00m)\n",
      "\n",
      "\u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33mRaw data processed from input files\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33mRamdomly sampling 100 rows for topic extraction\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "df_sample=raw_df.sample(n=\u001b[34m100\u001b[39;49;00m)\n",
      "sample_list = \u001b[36mlist\u001b[39;49;00m()\n",
      "\n",
      "\n",
      "\u001b[34mfor\u001b[39;49;00m index,row \u001b[35min\u001b[39;49;00m df_sample.iterrows():\n",
      "    entities = cm.detect_entities(Text=row[\u001b[33m'\u001b[39;49;00m\u001b[33mreview\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    topic_list = []\n",
      "    \u001b[34mfor\u001b[39;49;00m entity \u001b[35min\u001b[39;49;00m entities[\u001b[33m'\u001b[39;49;00m\u001b[33mEntities\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]:\n",
      "        \u001b[34mif\u001b[39;49;00m entity[\u001b[33m'\u001b[39;49;00m\u001b[33mCategory\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] == \u001b[33m'\u001b[39;49;00m\u001b[33mMEDICAL_CONDITION\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n",
      "            topic_list.append(entity[\u001b[33m'\u001b[39;49;00m\u001b[33mText\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "\n",
      "    sample_list.append({\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mid\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: row[\u001b[33m'\u001b[39;49;00m\u001b[33mid\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m],\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mdrugName\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: row[\u001b[33m'\u001b[39;49;00m\u001b[33mdrugName\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m],\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mcondition\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: row[\u001b[33m'\u001b[39;49;00m\u001b[33mcondition\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m],\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mreview\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: row[\u001b[33m'\u001b[39;49;00m\u001b[33mreview\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m],\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mtopics\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: topic_list[:\u001b[34m5\u001b[39;49;00m]\n",
      "        })\n",
      "        \n",
      "sample_df=pd.DataFrame.from_records(sample_list)\n",
      "\n",
      "sample_df.to_csv(\u001b[33m'\u001b[39;49;00m\u001b[33mprocessed_data/sample_df.csv\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, index=\u001b[34mFalse\u001b[39;49;00m) \n",
      "\n",
      "\n",
      "sampled_topics=pd.read_csv(\u001b[33m'\u001b[39;49;00m\u001b[33mprocessed_data/sample_df.csv\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)[\u001b[33m'\u001b[39;49;00m\u001b[33mtopics\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].tolist()\n",
      "\u001b[36mprint\u001b[39;49;00m(sampled_topics)\n",
      "vectorizer = TfidfVectorizer()\n",
      "vecs = vectorizer.fit_transform(sampled_topics)\n",
      "normalizer = Normalizer(copy=\u001b[34mFalse\u001b[39;49;00m)\n",
      "normalized_data = normalizer.fit_transform(vecs).toarray()\n",
      "normalized_data.shape\n",
      "np.savetxt(\u001b[33m\"\u001b[39;49;00m\u001b[33mprocessed_data/prediction_data.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, normalized_data, delimiter=\u001b[33m\"\u001b[39;49;00m\u001b[33m,\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "s3_client.upload_file(\u001b[33m'\u001b[39;49;00m\u001b[33mprocessed_data/sample_df.csv\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, bucket, \u001b[33m'\u001b[39;49;00m\u001b[33mchapter9/data/sample_df.csv\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "s3_client.upload_file(\u001b[33m'\u001b[39;49;00m\u001b[33mprocessed_data/raw_df.csv\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, bucket, \u001b[33m'\u001b[39;49;00m\u001b[33mchapter9/data/raw_df.csv\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "s3_client.upload_file(\u001b[33m'\u001b[39;49;00m\u001b[33mprocessed_data/prediction_data.csv\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, bucket, \u001b[33m'\u001b[39;49;00m\u001b[33mchapter9/data/prediction_data.csv\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "\n",
      "\u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33mprocessed files uploaded to s3\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n"
     ]
    }
   ],
   "source": [
    "!pygmentize scripts/preprocessing.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7460b9d5"
   },
   "source": [
    "As you can see from the script, we upload three files to S3 at the end of preprocessing. Examine the preprocessing code to make sure you understand how we are processing the raw data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5d381b1c"
   },
   "source": [
    "We create a docker container to run our preprocessing step. Let's look at the details of the container by examining the Dockerfile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bc4f6d3e",
    "outputId": "b2b93060-0b0f-4c97-ace6-173fedf62c16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mFROM\u001b[39;49;00m \u001b[33mpython:3.7-slim-buster\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mRUN\u001b[39;49;00m pip install pandas\n",
      "\u001b[34mRUN\u001b[39;49;00m pip install wget\n",
      "\u001b[34mRUN\u001b[39;49;00m pip install boto3\n",
      "\u001b[34mRUN\u001b[39;49;00m pip install sagemaker\n",
      "\u001b[34mRUN\u001b[39;49;00m pip install scikit-learn\n",
      "\n",
      "\u001b[37m# Make sure python doesn't buffer stdout so we get logs ASAP.\u001b[39;49;00m\n",
      "\u001b[34mENV\u001b[39;49;00m \u001b[31mPYTHONUNBUFFERED\u001b[39;49;00m=TRUE\n",
      "\u001b[34mENTRYPOINT\u001b[39;49;00m [\u001b[33m\"python3\"\u001b[39;49;00m]\n"
     ]
    }
   ],
   "source": [
    "!pygmentize scripts/Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2e00381b"
   },
   "source": [
    "Next, we build a docker container using the following shell script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d29cf05b",
    "outputId": "9423a9e9-49cf-408a-bee4-ba00868b6d65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "485822383573\n",
      "Login Succeeded\n",
      "\n",
      "Step 1/8 : FROM python:3.7-slim-buster\n",
      " ---> 8fe6e55c0412\n",
      "Step 2/8 : RUN pip install pandas\n",
      " ---> Using cache\n",
      " ---> ed3c2aadaa6e\n",
      "Step 3/8 : RUN pip install wget\n",
      " ---> Using cache\n",
      " ---> 93dc76d1c100\n",
      "Step 4/8 : RUN pip install boto3\n",
      " ---> Using cache\n",
      " ---> 43acfff1ec93\n",
      "Step 5/8 : RUN pip install sagemaker\n",
      " ---> Using cache\n",
      " ---> 0a0768240618\n",
      "Step 6/8 : RUN pip install scikit-learn\n",
      " ---> Using cache\n",
      " ---> 2ce6c8fc1e49\n",
      "Step 7/8 : ENV PYTHONUNBUFFERED=TRUE\n",
      " ---> Using cache\n",
      " ---> f68470613295\n",
      "Step 8/8 : ENTRYPOINT [\"python3\"]\n",
      " ---> Using cache\n",
      " ---> 41fbb5b0e27c\n",
      "Successfully built 41fbb5b0e27c\n",
      "Successfully tagged sagemaker-preprocessing:latest\n",
      "The push refers to repository [485822383573.dkr.ecr.us-east-1.amazonaws.com/sagemaker-preprocessing]\n",
      "20e4ef78b000: Preparing\n",
      "1cd08d11abf2: Preparing\n",
      "073fe9ab5fca: Preparing\n",
      "873a3963f49c: Preparing\n",
      "32682a294d34: Preparing\n",
      "cd77cebc5d3e: Preparing\n",
      "c899963fae46: Preparing\n",
      "353cc9dc1c96: Preparing\n",
      "c89d0deb3e29: Preparing\n",
      "735956b91a18: Preparing\n",
      "c89d0deb3e29: Waiting\n",
      "cd77cebc5d3e: Waiting\n",
      "735956b91a18: Waiting\n",
      "353cc9dc1c96: Waiting\n",
      "c899963fae46: Waiting\n",
      "073fe9ab5fca: Layer already exists\n",
      "20e4ef78b000: Layer already exists\n",
      "32682a294d34: Layer already exists\n",
      "1cd08d11abf2: Layer already exists\n",
      "873a3963f49c: Layer already exists\n",
      "cd77cebc5d3e: Layer already exists\n",
      "c899963fae46: Layer already exists\n",
      "c89d0deb3e29: Layer already exists\n",
      "353cc9dc1c96: Layer already exists\n",
      "735956b91a18: Layer already exists\n",
      "latest: digest: sha256:286e052a69cc9631505acf4a0051eb03e2229f2a616a514ea994839acda822fe size: 2428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "docker_name=sagemaker-preprocessing\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "echo $account\n",
    "region=$(aws configure get region)\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${docker_name}:latest\"\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "aws ecr describe-repositories --repository-names \"${docker_name}\" > /dev/null 2>&1\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${docker_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "$(aws ecr get-login --region ${region} --no-include-email)\n",
    "docker build -t $docker_name -f scripts/Dockerfile .\n",
    "docker tag ${docker_name} ${fullname}\n",
    "docker push ${fullname}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "887ce72e"
   },
   "source": [
    "We are now ready to define our steps in the pipeline. Let's begin by defining the preprocessing step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1a6d621a",
    "outputId": "cb55aae2-0fa8-455d-8e08-d590393fc519"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "485822383573.dkr.ecr.us-east-1.amazonaws.com/sagemaker-preprocessing:latest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/workflow/pipeline_context.py:198: UserWarning: Running within a PipelineSession, there will be No Wait, No Logs, and No Job being started.\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  sagemaker-preprocessing-2022-08-16-01-24-34-435\n",
      "Inputs:  [{'InputName': 'code', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-485822383573/sagemaker-preprocessing-2022-08-16-01-24-34-435/input/code/preprocessing.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  []\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.processing import ScriptProcessor\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "docker_name = \"sagemaker-preprocessing\"\n",
    "account = sagemaker_session.boto_session.client(\"sts\").get_caller_identity()[\"Account\"]\n",
    "region = sagemaker_session.boto_session.region_name\n",
    "image = \"{}.dkr.ecr.{}.amazonaws.com/{}:latest\".format(account, region, docker_name)\n",
    "print(image)\n",
    "script_processor = ScriptProcessor(image_uri=image,\n",
    "                role=role,\n",
    "                instance_count=1,\n",
    "                instance_type='ml.m5.xlarge',\n",
    "                command=['python3'],\n",
    "                sagemaker_session=pipeline_session)\n",
    "\n",
    "\n",
    "processor_args=script_processor.run(code='scripts/preprocessing.py',\n",
    "                    arguments = [\"--bucket\",bucket,'--region',region])\n",
    "\n",
    "\n",
    "\n",
    "step_process = ProcessingStep(\n",
    "    name=\"PreprocessData\",\n",
    "    step_args=processor_args,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c64e641b"
   },
   "source": [
    "Just like the preprocessing script, we have a training script that we will use to train our model. Let's look at that in more detail. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "098d220d",
    "outputId": "2f004db4-bb8f-4e04-9350-1bb6b7923a6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mcluster\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m KMeans\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mexternals\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m joblib\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mpreprocessing\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Normalizer\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mfeature_extraction\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mtext\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m TfidfVectorizer\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mio\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m StringIO\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel_fn\u001b[39;49;00m(model_dir):\n",
      "    \n",
      "    kmeans = joblib.load(os.path.join(model_dir, \u001b[33m\"\u001b[39;49;00m\u001b[33mkmeansmodel.joblib\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))\n",
      "    \u001b[34mreturn\u001b[39;49;00m kmeans\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32minput_fn\u001b[39;49;00m(input_data, content_type):\n",
      "    \n",
      "    \u001b[34mif\u001b[39;49;00m content_type == \u001b[33m\"\u001b[39;49;00m\u001b[33mtext/csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "        \u001b[37m# Read the raw input data as CSV.\u001b[39;49;00m\n",
      "        df = pd.read_csv(StringIO(input_data), header=\u001b[34mNone\u001b[39;49;00m)\n",
      "        \u001b[34mreturn\u001b[39;49;00m df\n",
      "    \u001b[34melse\u001b[39;49;00m:\n",
      "        \u001b[34mraise\u001b[39;49;00m \u001b[36mValueError\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m not supported by script!\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(content_type))\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m'\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n",
      "    parser = argparse.ArgumentParser()\n",
      "    \n",
      "    \u001b[37m# Hyperparameters are described here. In this simple example we are just including one hyperparameter.\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--n_clusters\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m2\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--random_state\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m0\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[37m# Sagemaker specific arguments. Defaults are set in the environment variables.\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--output-data-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_OUTPUT_DATA_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--model-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--training\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAINING\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "\n",
      "    args = parser.parse_args()\n",
      "    \n",
      "    input_files = [ os.path.join(args.training, file) \u001b[34mfor\u001b[39;49;00m file \u001b[35min\u001b[39;49;00m os.listdir(args.training) ]\n",
      "    \u001b[34mif\u001b[39;49;00m \u001b[36mlen\u001b[39;49;00m(input_files) == \u001b[34m0\u001b[39;49;00m:\n",
      "        \u001b[34mraise\u001b[39;49;00m \u001b[36mValueError\u001b[39;49;00m((\u001b[33m'\u001b[39;49;00m\u001b[33mThere are no files in \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m.\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m +\n",
      "                          \u001b[33m'\u001b[39;49;00m\u001b[33mThis usually indicates that the channel (\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m) was incorrectly specified,\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m +\n",
      "                          \u001b[33m'\u001b[39;49;00m\u001b[33mthe data specification in S3 was incorrectly specified or the role specified\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m +\n",
      "                          \u001b[33m'\u001b[39;49;00m\u001b[33mdoes not have permission to access the data.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).format(args.training, \u001b[33m\"\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))\n",
      "    \n",
      "    raw_data = [ pd.read_csv(file) \u001b[34mfor\u001b[39;49;00m file \u001b[35min\u001b[39;49;00m input_files ]\n",
      "    train_data = pd.concat(raw_data)\n",
      "    \u001b[36mprint\u001b[39;49;00m(train_data.shape)\n",
      "    kmeans = KMeans(n_clusters=\u001b[34m2\u001b[39;49;00m,random_state=\u001b[34m0\u001b[39;49;00m).fit(train_data)\n",
      "    joblib.dump(kmeans, os.path.join(args.model_dir, \u001b[33m\"\u001b[39;49;00m\u001b[33mkmeansmodel.joblib\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))\n"
     ]
    }
   ],
   "source": [
    "!pygmentize scripts/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b961628b"
   },
   "source": [
    "As you can see, the script trains a `kmeans` clustering model with two clusters. We will now add a training step to our pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "21132e47"
   },
   "outputs": [],
   "source": [
    "from sagemaker.workflow.steps import TrainingStep\n",
    "\n",
    "model_path= f\"s3://{bucket}/{prefix}/model/\"\n",
    "\n",
    "\n",
    "sklearn = SKLearn(\n",
    "    source_dir='scripts',\n",
    "    entry_point='train.py',\n",
    "    instance_type=\"ml.m4.xlarge\",\n",
    "    role = role,\n",
    "    sagemaker_session=pipeline_session,\n",
    "    framework_version='0.20.0',\n",
    "    output_path=model_path,\n",
    "    hyperparameters={'n_clusters': 2, 'random_state':0})\n",
    "\n",
    "train_args=sklearn.fit({'training': 's3://{}/{}/prediction_data.csv'.format(bucket,prefix)})\n",
    "step_train_model = TrainingStep(name=\"TrainModel\", step_args=train_args)\n",
    "step_train_model.add_depends_on([step_process])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "34e1a0b6"
   },
   "source": [
    "We are now ready to register our model to SageMaker model registry. This is done in the following code block. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d6687f5e"
   },
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "from sagemaker.sklearn.model import SKLearnModel\n",
    "from sagemaker.workflow.model_step import ModelStep\n",
    "\n",
    "\n",
    "\n",
    "clustering_model = SKLearnModel(\n",
    "    model_data=step_train_model.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    role=role,\n",
    "    sagemaker_session=pipeline_session,\n",
    "    entry_point=\"scripts/train.py\",\n",
    "    framework_version='0.20.0',\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "register_model_step_args = clustering_model.register(\n",
    "    content_types=[\"text/csv\"],\n",
    "   response_types=[\"text/csv\"],\n",
    "   inference_instances=[\"ml.t2.medium\"],\n",
    "   model_package_group_name='adverse-event-clustering'\n",
    ")\n",
    "\n",
    "step_register=ModelStep(name='adverse-event-clustering-model', step_args=register_model_step_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "81402ff8"
   },
   "source": [
    "Next, we add the three sequential steps into a pipeline and look at the pipeline definition.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "abcc23fb"
   },
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=\"adverse-drug-reaction\",\n",
    "    steps=[step_process, step_train_model, step_register]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "138e4eba",
    "outputId": "925696b9-6a81-473d-961b-3694d8890281"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Version': '2020-12-01',\n",
       " 'Metadata': {},\n",
       " 'Parameters': [],\n",
       " 'PipelineExperimentConfig': {'ExperimentName': {'Get': 'Execution.PipelineName'},\n",
       "  'TrialName': {'Get': 'Execution.PipelineExecutionId'}},\n",
       " 'Steps': [{'Name': 'PreprocessData',\n",
       "   'Type': 'Processing',\n",
       "   'Arguments': {'ProcessingResources': {'ClusterConfig': {'InstanceType': 'ml.m5.xlarge',\n",
       "      'InstanceCount': 1,\n",
       "      'VolumeSizeInGB': 30}},\n",
       "    'AppSpecification': {'ImageUri': '485822383573.dkr.ecr.us-east-1.amazonaws.com/sagemaker-preprocessing:latest',\n",
       "     'ContainerArguments': ['--bucket',\n",
       "      'sagemaker-us-east-1-485822383573',\n",
       "      '--region',\n",
       "      'us-east-1'],\n",
       "     'ContainerEntrypoint': ['python3',\n",
       "      '/opt/ml/processing/input/code/preprocessing.py']},\n",
       "    'RoleArn': 'arn:aws:iam::485822383573:role/service-role/AmazonSageMaker-ExecutionRole-20220426T122295',\n",
       "    'ProcessingInputs': [{'InputName': 'code',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-485822383573/sagemaker-preprocessing-2022-08-16-01-24-34-435/input/code/preprocessing.py',\n",
       "       'LocalPath': '/opt/ml/processing/input/code',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}}]}},\n",
       "  {'Name': 'TrainModel',\n",
       "   'Type': 'Training',\n",
       "   'Arguments': {'AlgorithmSpecification': {'TrainingInputMode': 'File',\n",
       "     'TrainingImage': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:0.20.0-cpu-py3'},\n",
       "    'OutputDataConfig': {'S3OutputPath': 's3://sagemaker-us-east-1-485822383573/chapter9/data/model/'},\n",
       "    'StoppingCondition': {'MaxRuntimeInSeconds': 86400},\n",
       "    'ResourceConfig': {'VolumeSizeInGB': 30,\n",
       "     'InstanceCount': 1,\n",
       "     'InstanceType': 'ml.m4.xlarge'},\n",
       "    'RoleArn': 'arn:aws:iam::485822383573:role/service-role/AmazonSageMaker-ExecutionRole-20220426T122295',\n",
       "    'InputDataConfig': [{'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix',\n",
       "        'S3Uri': 's3://sagemaker-us-east-1-485822383573/chapter9/data/prediction_data.csv',\n",
       "        'S3DataDistributionType': 'FullyReplicated'}},\n",
       "      'ChannelName': 'training'}],\n",
       "    'HyperParameters': {'n_clusters': '2',\n",
       "     'random_state': '0',\n",
       "     'sagemaker_submit_directory': '\"s3://sagemaker-us-east-1-485822383573/sagemaker-scikit-learn-2022-08-16-01-24-36-632/source/sourcedir.tar.gz\"',\n",
       "     'sagemaker_program': '\"train.py\"',\n",
       "     'sagemaker_container_log_level': '20',\n",
       "     'sagemaker_region': '\"us-east-1\"'},\n",
       "    'DebugHookConfig': {'S3OutputPath': 's3://sagemaker-us-east-1-485822383573/chapter9/data/model/',\n",
       "     'CollectionConfigurations': []},\n",
       "    'ProfilerRuleConfigurations': [{'RuleConfigurationName': 'ProfilerReport-1660613076',\n",
       "      'RuleEvaluatorImage': '503895931360.dkr.ecr.us-east-1.amazonaws.com/sagemaker-debugger-rules:latest',\n",
       "      'RuleParameters': {'rule_to_invoke': 'ProfilerReport'}}],\n",
       "    'ProfilerConfig': {'S3OutputPath': 's3://sagemaker-us-east-1-485822383573/chapter9/data/model/'}},\n",
       "   'DependsOn': ['PreprocessData']},\n",
       "  {'Name': 'adverse-event-clustering-model-RegisterModel',\n",
       "   'Type': 'RegisterModel',\n",
       "   'Arguments': {'ModelPackageGroupName': 'adverse-event-clustering',\n",
       "    'InferenceSpecification': {'Containers': [{'Image': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:0.20.0-cpu-py3',\n",
       "       'Environment': {'SAGEMAKER_PROGRAM': 'train.py',\n",
       "        'SAGEMAKER_SUBMIT_DIRECTORY': 's3://sagemaker-us-east-1-485822383573/sagemaker-scikit-learn-2022-08-16-01-24-38-454/sourcedir.tar.gz',\n",
       "        'SAGEMAKER_CONTAINER_LOG_LEVEL': '20',\n",
       "        'SAGEMAKER_REGION': 'us-east-1'},\n",
       "       'ModelDataUrl': {'Get': 'Steps.TrainModel.ModelArtifacts.S3ModelArtifacts'},\n",
       "       'Framework': 'SKLEARN',\n",
       "       'FrameworkVersion': '0.20.0'}],\n",
       "     'SupportedContentTypes': ['text/csv'],\n",
       "     'SupportedResponseMIMETypes': ['text/csv'],\n",
       "     'SupportedRealtimeInferenceInstanceTypes': ['ml.t2.medium']},\n",
       "    'ModelApprovalStatus': 'PendingManualApproval'}}]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "definition = json.loads(pipeline.definition())\n",
    "definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6acc03d"
   },
   "source": [
    "We are now ready to start our pipeline execution. The next few lines of code begin the pipeline execution and examines its status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "92b92194",
    "outputId": "876c1bbf-46d8-4a98-841a-3fcc75701662"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:us-east-1:485822383573:pipeline/adverse-drug-reaction',\n",
       " 'ResponseMetadata': {'RequestId': '21caa611-b96e-4e37-af34-b6c47e70e945',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '21caa611-b96e-4e37-af34-b6c47e70e945',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '89',\n",
       "   'date': 'Tue, 16 Aug 2022 01:25:25 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.upsert(role_arn=role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ed5d01a8"
   },
   "outputs": [],
   "source": [
    "execution = pipeline.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "33e85a59",
    "outputId": "89d9fe3d-52ff-4f30-d780-66a0c9954273"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:us-east-1:485822383573:pipeline/adverse-drug-reaction',\n",
       " 'PipelineExecutionArn': 'arn:aws:sagemaker:us-east-1:485822383573:pipeline/adverse-drug-reaction/execution/7hzfj8g9p4o7',\n",
       " 'PipelineExecutionDisplayName': 'execution-1660613127172',\n",
       " 'PipelineExecutionStatus': 'Executing',\n",
       " 'PipelineExperimentConfig': {'ExperimentName': 'adverse-drug-reaction',\n",
       "  'TrialName': '7hzfj8g9p4o7'},\n",
       " 'CreationTime': datetime.datetime(2022, 8, 16, 1, 25, 27, 94000, tzinfo=tzlocal()),\n",
       " 'LastModifiedTime': datetime.datetime(2022, 8, 16, 1, 25, 27, 94000, tzinfo=tzlocal()),\n",
       " 'CreatedBy': {},\n",
       " 'LastModifiedBy': {},\n",
       " 'ResponseMetadata': {'RequestId': '4451c759-bd77-48fc-be65-cf2a383ec9d7',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '4451c759-bd77-48fc-be65-cf2a383ec9d7',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '504',\n",
       "   'date': 'Tue, 16 Aug 2022 01:25:28 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execution.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2d7480ad"
   },
   "outputs": [],
   "source": [
    "execution.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fb3b141b"
   },
   "source": [
    "The pipeline is now running. At the end of this run, you will have a model registered in SageMaker model registry. Leave the notebook running. Return to *Chapter 9* in the book for instructions to complete the remaining steps in this exercise."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "adverse-reaction-pipeline.ipynb",
   "provenance": []
  },
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
